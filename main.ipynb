{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp_20220614\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchtext\n",
    "# !pip install torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_num = cpu_count() # 自动获取最大核心数目\n",
    "os.environ ['OMP_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['OPENBLAS_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['MKL_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['VECLIB_MAXIMUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['NUMEXPR_NUM_THREADS'] = str(cpu_num)\n",
    "torch.set_num_threads(cpu_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the Text data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 9 at dim 1 (got 34)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [60]\u001B[0m, in \u001B[0;36m<cell line: 109>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    104\u001B[0m data_prepare\u001B[38;5;241m=\u001B[39mData_Prepare()\n\u001B[0;32m    106\u001B[0m \u001B[38;5;66;03m# train_X, train_y=data_prepare.extract_word_labels(tran_path)\u001B[39;00m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m# dev_X, dev_y=data_prepare.extract_word_labels(dev_path)\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# test_X, test_y=data_prepare.extract_word_labels(test_path)\u001B[39;00m\n\u001B[1;32m--> 109\u001B[0m train_dataset\u001B[38;5;241m=\u001B[39m\u001B[43mdata_prepare\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_word_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtran_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m dev_dataset\u001B[38;5;241m=\u001B[39mdata_prepare\u001B[38;5;241m.\u001B[39mextract_word_labels(dev_path)\n\u001B[0;32m    111\u001B[0m test_dataset\u001B[38;5;241m=\u001B[39mdata_prepare\u001B[38;5;241m.\u001B[39mextract_word_labels(test_path)\n",
      "Input \u001B[1;32mIn [60]\u001B[0m, in \u001B[0;36mData_Prepare.extract_word_labels\u001B[1;34m(self, filepath)\u001B[0m\n\u001B[0;32m     50\u001B[0m         label\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m     52\u001B[0m sentences_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentence_embedding(sentences)\n\u001B[1;32m---> 54\u001B[0m data_tensor\u001B[38;5;241m=\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m labels_tensor\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mTensor(np\u001B[38;5;241m.\u001B[39marray(labels))\n\u001B[0;32m     56\u001B[0m dataset\u001B[38;5;241m=\u001B[39mTensorDataset(data_tensor,labels_tensor)\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 9 at dim 1 (got 34)"
     ]
    }
   ],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data_tensor, labels_tensor):\n",
    "        self.data_tensor=data_tensor\n",
    "        self.labels_tensor=labels_tensor\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data_tensor[item], self.labels_tensor[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "class Data_Prepare:\n",
    "    def __init__(self):\n",
    "        # public variables\n",
    "        self.EMBED_DIM=50\n",
    "        self.vocab=self.load_glove_model('word_embedding/glove.6B.50d/glove.6B.50d.txt')\n",
    "\n",
    "    def load_glove_model(self,File):\n",
    "        print(\"Loading Glove Model\")\n",
    "        glove_model = {}\n",
    "        with open(File,'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                glove_model[word] = embedding\n",
    "        print(f\"{len(glove_model)} words loaded!\")\n",
    "        return glove_model\n",
    "\n",
    "    def extract_word_labels(self, filepath):\n",
    "        df=pd.read_csv(filepath,delimiter='\\t',names=['Word','POS','NP','NER'],skiprows=[0])\n",
    "        # df=self.word_embedding(df)\n",
    "        # labels=self.onehot_encode(df.NER)\n",
    "\n",
    "        sentences=[]\n",
    "        labels=[]\n",
    "        label=[]\n",
    "        sentence=[]\n",
    "        tag_dict={}\n",
    "        for index, item in enumerate(set(df.NER)):\n",
    "            tag_dict[item]=index\n",
    "        for word,tag in zip(df.Word,df.NER):\n",
    "            tag_id=tag_dict.get(tag)\n",
    "            label.append(tag_id)\n",
    "            sentence.append(word)\n",
    "            if word =='.':\n",
    "                labels.append(label)\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "                label=[]\n",
    "\n",
    "        sentences_embeddings=self.sentence_embedding(sentences)\n",
    "\n",
    "        data_tensor=torch.Tensor(sentences_embeddings)\n",
    "        labels_tensor=torch.Tensor(np.array(labels))\n",
    "        dataset=TensorDataset(data_tensor,labels_tensor)\n",
    "\n",
    "        # word_embeddings=np.array(df.Embedding.to_list())\n",
    "        # labels_tensor=torch.Tensor(np.array(labels))\n",
    "        # data_tensor=torch.Tensor(word_embeddings)\n",
    "        # dataset=TensorDataset(data_tensor,labels_tensor)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def sentence_embedding(self, sentences):\n",
    "        sents_embeddings=[]\n",
    "        for sent in sentences:\n",
    "            sent_embeddings=[]\n",
    "            for word in sent:\n",
    "                vector=self.vocab.get(str(word).lower())\n",
    "                if vector is not None:\n",
    "                    sent_embeddings.append(vector)\n",
    "                else:\n",
    "                    sent_embeddings.append(np.zeros(self.EMBED_DIM))\n",
    "            sents_embeddings.append(sent_embeddings)\n",
    "        return sents_embeddings\n",
    "\n",
    "\n",
    "    def word_embedding(self,df):\n",
    "        embeddings=[]\n",
    "        for word in df.Word:\n",
    "            vector=self.vocab.get(str(word).lower())\n",
    "            if vector is not None:\n",
    "                embeddings.append(vector)\n",
    "            else:\n",
    "                embeddings.append(np.zeros(self.EMBED_DIM))\n",
    "\n",
    "        df['Embedding']=embeddings\n",
    "        return df\n",
    "\n",
    "    def onehot_encode(self, labels):\n",
    "        labels_to_ids={k:v for v,k in enumerate(set(labels.to_list()))}\n",
    "        result=[]\n",
    "        for label in labels:\n",
    "            vec=np.zeros(len(labels_to_ids))\n",
    "            vec[labels_to_ids.get(label)]=1\n",
    "            result.append(vec)\n",
    "        return result\n",
    "\n",
    "tran_path='data/train.conll'\n",
    "dev_path='data/dev.conll'\n",
    "test_path='data/test.conll'\n",
    "\n",
    "data_prepare=Data_Prepare()\n",
    "\n",
    "# train_X, train_y=data_prepare.extract_word_labels(tran_path)\n",
    "# dev_X, dev_y=data_prepare.extract_word_labels(dev_path)\n",
    "# test_X, test_y=data_prepare.extract_word_labels(test_path)\n",
    "train_dataset=data_prepare.extract_word_labels(tran_path)\n",
    "dev_dataset=data_prepare.extract_word_labels(dev_path)\n",
    "test_dataset=data_prepare.extract_word_labels(test_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tensor_dataloader = DataLoader(train_dataset,\n",
    "                               batch_size=2,\n",
    "                               shuffle=True,\n",
    "                               num_workers=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp_20220614\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, params:dict):\n",
    "        super(RNN,self).__init__()\n",
    "        self.num_layers=params['num_layers']\n",
    "        self.hidden_size=params['hidden_size']\n",
    "        self.batch_size=params['batch_size']\n",
    "        self.bilstm=nn.LSTM(input_size=params['input_size'],\n",
    "                            hidden_size=params['hidden_size']//2,\n",
    "                            num_layers=params['num_layers'],\n",
    "                            batch_first=True,\n",
    "                            dropout=params['dropout'],\n",
    "                            bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.classifier=nn.Linear(params['hidden_size'],params['num_classes'])\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # self.hidden = self.init_hidden()\n",
    "        out, _=self.bilstm(x)\n",
    "        out=out.view(-1,self.hidden_size)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers*2,self.batch_size,self.hidden_size//2),\n",
    "                torch.zeros(self.num_layers*2,self.batch_size,self.hidden_size//2))\n",
    "\n",
    "\n",
    "params={\n",
    "    'num_layers':1,\n",
    "    'hidden_size': 100,\n",
    "    'input_size':50,\n",
    "    'learning_rate':0.01,\n",
    "    'optimizer':'adam',\n",
    "    'epochs':10,\n",
    "    'batch_size':1,\n",
    "    'num_classes':10,\n",
    "    'dropout':0.1\n",
    "}\n",
    "rnn=RNN(params=params)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(),lr=params['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "train_mini_set=train_dataset[:10000]\n",
    "train_mini_set=TensorDataset(train_mini_set[0],train_mini_set[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "204566"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.4590279463598904\n",
      "Macro average F1_score is0.8549159509083506\n",
      "\n",
      "Epochs: 0 / 10: Training Loss:0.20784056379302482 \t\t Validation Loss:2.6022185808163976e-05\n",
      "Validation Loss Decreased(inf--->1.342146) \t Saving The Model\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.46293739620292385\n",
      "Macro average F1_score is0.8554200515733758\n",
      "\n",
      "Epochs: 1 / 10: Training Loss:0.21082767813749206 \t\t Validation Loss:2.4449518578977546e-05\n",
      "Validation Loss Decreased(1.342146--->1.261033) \t Saving The Model\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.4623176871756421\n",
      "Macro average F1_score is0.8559435407255171\n",
      "\n",
      "Epochs: 2 / 10: Training Loss:0.21370962603976695 \t\t Validation Loss:2.4915585136151013e-05\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.45963746906172964\n",
      "Macro average F1_score is0.8520464548151307\n",
      "\n",
      "Epochs: 3 / 10: Training Loss:0.2197484414287851 \t\t Validation Loss:2.4369887781279094e-05\n",
      "Validation Loss Decreased(1.261033--->1.256926) \t Saving The Model\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.47620326115361167\n",
      "Macro average F1_score is0.8591038641254823\n",
      "\n",
      "Epochs: 4 / 10: Training Loss:0.21453357597245648 \t\t Validation Loss:2.4138594902386635e-05\n",
      "Validation Loss Decreased(1.256926--->1.244996) \t Saving The Model\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.4776460500653153\n",
      "Macro average F1_score is0.8582507706923629\n",
      "\n",
      "Epochs: 5 / 10: Training Loss:0.22066255012819455 \t\t Validation Loss:2.4703778724486245e-05\n",
      "Model training:[*************************************************************************************************** ]99%\n",
      "Macro average F1_score is0.4703412511419707\n",
      "Macro average F1_score is0.8570292960040327\n",
      "\n",
      "Epochs: 6 / 10: Training Loss:0.20747393475626547 \t\t Validation Loss:3.033267510303163e-05\n",
      "Model training:[***                                                                                                 ]3%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 22>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m     data, label \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mcuda(), label\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m     30\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Clear the gradients\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m predict\u001B[38;5;241m=\u001B[39m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Forward Pass\u001B[39;00m\n\u001B[0;32m     32\u001B[0m loss\u001B[38;5;241m=\u001B[39mcriterion(predict,label)  \u001B[38;5;66;03m# Find the Loss\u001B[39;00m\n\u001B[0;32m     33\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# Calculate gradients\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp_20220614\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mRNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# self.hidden = self.init_hidden()\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m     out, _\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbilstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     out\u001B[38;5;241m=\u001B[39mout\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\n\u001B[0;32m     24\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(out)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp_20220614\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp_20220614\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    759\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[0;32m    760\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 761\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    763\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    764\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m    765\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_bar(num, total):\n",
    "    rate = float(num)/total\n",
    "    ratenum = int(100*rate)\n",
    "    r = '\\rModel training:[{}{}]{}%'.format('*'*ratenum,' '*(100-ratenum), ratenum)\n",
    "    sys.stdout.write(r)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "train_dataloader = DataLoader(train_mini_set,\n",
    "                           batch_size=1,\n",
    "                           shuffle=False,\n",
    "                           num_workers=0)\n",
    "\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                       batch_size=1,\n",
    "                       shuffle=False,\n",
    "                       num_workers=0)\n",
    "\n",
    "step=int(0.01*len(train_mini_set))\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss=0.\n",
    "\n",
    "    progress=0.\n",
    "    for data, label in train_dataloader:\n",
    "        progress+=1\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        predict=rnn(data) # Forward Pass\n",
    "        loss=criterion(predict,label)  # Find the Loss\n",
    "        loss.backward()  # Calculate gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        train_loss +=loss.item()\n",
    "        if progress % step==1:\n",
    "            process_bar(progress,len(train_mini_set))\n",
    "\n",
    "    valid_loss=0.\n",
    "    rnn.eval()\n",
    "    targets=[]\n",
    "    golds=[]\n",
    "    for data, label in dev_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "\n",
    "        target=rnn(data)\n",
    "        loss=criterion(target,label)\n",
    "        valid_loss=loss.item()*data.size(0)\n",
    "\n",
    "        targets.append(torch.argmax(target).item())\n",
    "        golds.append(torch.argmax(label).item())\n",
    "\n",
    "    f1_scores_macro=f1_score(golds,targets,average='macro')\n",
    "    print(\"\\nMacro average F1_score is{0}\".format(f1_scores_macro))\n",
    "\n",
    "    f1_score_micro=f1_score(golds,targets,average='micro')\n",
    "    print(\"Micro average F1_score is{0}\".format(f1_score_micro))\n",
    "\n",
    "\n",
    "    print(\"\\nEpochs: {0} / {1}: Training Loss:{2} \\t\\t Validation Loss:{3}\"\n",
    "          .format(epoch,params['epochs'],train_loss / len(train_dataloader), valid_loss/len(dev_dataset)))\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print('Validation Loss Decreased({0:.6f}--->{1:.6f}) \\t Saving The Model'.format(min_valid_loss,valid_loss))\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(rnn.state_dict(), 'saved_model.pth')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(rnn(train_dataset[2500][0].view(1,50))).item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-15.0669,  -1.4674,  -3.3839,  -3.5529,  -7.3759, -11.6590,  -5.4836,\n          -8.9921,  -6.8117, -10.7845]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(train_dataset[2500][0].view(1,50))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "num=1300\n",
    "if torch.argmax(rnn(train_dataset[num][0].view(1,50)))==torch.argmax(train_dataset[num][1]):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('wrong')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(baseline) on the test set is : 0.0179\n",
      "Accuracy(Model) on the test set is : 0.8499\n"
     ]
    }
   ],
   "source": [
    "num_correct=0\n",
    "for data, label in dev_dataset:\n",
    "    if torch.argmax(torch.Tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))==torch.argmax(label):\n",
    "        num_correct+=1\n",
    "print('Accuracy(baseline) on the test set is : {0:5.4f}'.format(num_correct/len(dev_dataset)))\n",
    "\n",
    "num_correct=0\n",
    "for data, label in dev_dataset:\n",
    "    if torch.argmax(rnn(data.view(1,50)))==torch.argmax(label):\n",
    "        num_correct+=1\n",
    "print('Accuracy(Model) on the test set is : {0:5.4f}'.format(num_correct/len(dev_dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4603, Accuracy: 37821/46665 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval() # 必备，将模型设置为评估模式\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "        for data, target in test_loader: # 从数据加载器迭代一个batch的数据\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            # print(output.max(1, keepdim=True)[1])\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.max(1, keepdim=True)[1]).sum().item() # 统计预测正确个数\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                           batch_size=1,\n",
    "                           shuffle=False,\n",
    "                           num_workers=0)\n",
    "test(model=rnn,device=device,test_loader=test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "RNN(\n  (bilstm): LSTM(50, 50, batch_first=True, dropout=0.1, bidirectional=True)\n  (classifier): Linear(in_features=100, out_features=10, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tran_pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m embeddings\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m      2\u001B[0m EMBED_DIM\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtran_pd\u001B[49m\u001B[38;5;241m.\u001B[39mWord:\n\u001B[0;32m      4\u001B[0m     vector\u001B[38;5;241m=\u001B[39mvocab\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mstr\u001B[39m(word)\u001B[38;5;241m.\u001B[39mlower())\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m vector \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tran_pd' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings=[]\n",
    "EMBED_DIM=50\n",
    "for word in tran_pd.Word:\n",
    "    vector=vocab.get(str(word).lower())\n",
    "    if vector is not None:\n",
    "        embeddings.append(vector)\n",
    "    else:\n",
    "        embeddings.append(np.zeros(EMBED_DIM))\n",
    "\n",
    "tran_pd['Embedding']=embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tran_pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels=tran_pd.NER\n",
    "\n",
    "\n",
    "def onehot_encode(labels):\n",
    "    labels_to_ids={k:v for v,k in enumerate(set(labels.to_list()))}\n",
    "    result=[]\n",
    "    for label in labels:\n",
    "        vec=np.zeros(len(labels_to_ids))\n",
    "        vec[labels_to_ids.get(label)]=1\n",
    "        result.append(vec)\n",
    "    return result\n",
    "\n",
    "labels_OHE=onehot_encode(labels) # label encoded in one hot format\n",
    "len(labels_OHE)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tran_X=tran_pd.Embedding.to_list()\n",
    "tran_y=labels_OHE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "filepath='data/train.conll'\n",
    "\n",
    "df=pd.read_csv(filepath,delimiter='\\t',names=['Word','POS','NP','NER'],skiprows=[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8122147219543555"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df.NER=='O'])/len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "label=[]\n",
    "sentence=[]\n",
    "tag_dict={}\n",
    "for index, item in enumerate(set(df.NER)):\n",
    "    tag_dict[item]=index\n",
    "for word,tag in zip(df.Word,df.NER):\n",
    "    tag_id=tag_dict.get(tag)\n",
    "    label.append(tag_id)\n",
    "    sentence.append(word)\n",
    "    if word =='.':\n",
    "        labels.append(label)\n",
    "        sentences.append(sentence)\n",
    "        sentence=[]\n",
    "        label=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(13, 13)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=52\n",
    "len(sentences[num]),len(labels[num])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[7, 9, 1, 9, 9, 9, 1, 9, 9]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUSSELS 1996-08-23 Leading results in the Brussels Grand Prix athletics meeting on Friday : Women 's discus 1. Ilke Wyludda ( Germany ) 66.60 metres 2. Ellina Zvereva ( Belarus ) 65.66 3. Franka Dietzsch ( Germany ) 61.74 4. Natalya Sadova ( Russia ) 61.64 5. Mette Bergmann ( Norway ) 61.44 6. Nicoleta Grasu ( Romania ) 61.36 7. Olga Chernyavskaya ( Russia ) 60.46 8. Irina Yatchenko ( Belarus ) 58.92 Women 's 100 metres hurdles 1. Ludmila Engquist ( Sweden ) 12.60 seconds 2. Michelle Freeman ( Jamaica ) 12.77 3. Aliuska Lopez ( Cuba ) 12.85 4. Dionne Rose ( Jamaica ) 12.88 5. Brigita Bukovec ( Slovakia ) 12.95 6. Yulia Graudin ( Russia ) 12.96 7. Julie Baumann ( Switzerland ) 13.36 8. Patricia Girard-Leno ( France ) 13.36 9. Dawn Bowles ( U.S. ) 13.53 Men 's 110 metres hurdles 1. Allen Johnson ( U.S. ) 12.92 seconds 2. Colin Jackson ( Britain ) 13.24 3. Emilio Valle ( Cuba ) 13.33 4. Sven Pieters ( Belgium ) 13.37 5. Steve Brown ( U.S. ) 13.38 6. Frank Asselman ( Belgium ) 13.64 7. Hubert Grossard ( Belgium ) 13.65 8. Jonathan N'Senga ( Belgium ) 13.66 9. Johan Lisabeth ( Belgium ) 13.75 Women 's 5,000 metres 1. Roberta Brunet ( Italy ) 14 minutes 48.96 seconds 2. Fernanda Ribeiro ( Portugal ) 14:49.81 3. Sally Barsosio ( Kenya ) 14:58.29 4. Paula Radcliffe ( Britain ) 14:59.70 5. Julia Vaquero ( Spain ) 15:04.94 6. Catherine McKiernan ( Ireland ) 15:07.57 7. Annette Peters ( U.S. ) 15:07.85 8. Pauline Konga ( Kenya ) 15:11.40 Men 's 100 metres 1. Dennis Mitchell ( U.S. ) 10.03 seconds 2. Donovan Bailey ( Canada ) 10.09 3. Carl Lewis ( U.S. ) 10.10 4. Ato Boldon ( Trinidad ) 10.12 5. Linford Christie ( Britain ) 10.14 6. Davidson Ezinwa ( Nigeria ) 10.15 7. Jon Drummond ( U.S. ) 10.16 8. Bruny Surin ( Canada ) 10.30 Men 's 400 metres hurdles 1. Derrick Adkins ( U.S. ) 47.93 seconds 2. Samuel Matete ( Zambia ) 47.99 3. Rohan Robinson ( Australia ) 48.86 4. Torrance Zellner ( U.S. ) 49.06 5. Jean-Paul Bruwier ( Belgium ) 49.24 6. Dusan Kovacs ( Hungary ) 49.31 7. Calvin Davis ( U.S. ) 49.49 8. Laurent Ottoz ( Italy ) 49.61 9. Marc Dollendorf ( Belgium ) 50.36 Women 's 100 metres 1. Gail Devers ( U.S. ) 10.84 seconds 2. Gwen Torrence ( U.S. ) 11.00 3. Merlene Ottey ( Jamaica ) 11.04 4. Mary Onyali ( Nigeria ) 11.09 5. Chryste Gaines ( U.S. ) 11.18 6. Zhanna Pintusevich ( Ukraine ) 11.27 7. Irina Privalova ( Russia ) 11.28 8. Natalia Voronova ( Russia ) 11.28 9. Juliet Cuthbert ( Jamaica ) 11.31 Women 's 1,500 metres 1. Regina Jacobs ( U.S. ) 4 minutes 01.77 seconds 2. Patricia Djate ( France ) 4:02.26 3. Carla Sacramento ( Portugal ) 4:02.67 4. Yekaterina Podkopayeva ( Russia ) 4:04.78 5. Margret Crowley ( Australia ) 4:05.00 6. Leah Pells ( Canada ) 4:05.64 7. Sarah Thorsett ( U.S. ) 4:06.80 8. Sinead Delahunty ( Ireland ) 4:07.27 3,000 metres steeplechase 1. Joseph Keter ( Kenya ) 8 minutes 10.02 seconds 2. Patrick Sang ( Kenya ) 8:12.04 3. Moses Kiptanui ( Kenya ) 8:12.65 4. Gideon Chirchir ( Kenya ) 8:15.69 5. Richard Kosgei ( Kenya ) 8:16.80 6. Larbi El Khattabi ( Morocco ) 8:17.29 7. Eliud Barngetuny ( Kenya ) 8:17.66 8. Bernard Barmasai ( Kenya ) 8:17.94 Men 's 400 metres 1. Michael Johnson ( U.S. ) 44.29 seconds 2. Derek Mills ( U.S. ) 44.78 3. Anthuan Maybank ( U.S. ) 44.92 4. Davis Kamoga ( Uganda ) 44.96 5. Jamie Baulch ( Britain ) 45.08 6. Sunday Bada ( Nigeria ) 45.21 7. Samson Kitur ( Kenya ) 45.34 8. Mark Richardson ( Britain ) 45.67 9. Jason Rouser ( U.S. ) 46.11 Men 's 200 metres 1. Frankie Fredericks ( Namibia ) 19.92 seconds 2. Ato Boldon ( Trinidad ) 19.99 3. Jeff Williams ( U.S. ) 20.21 4. Jon Drummond ( U.S. ) 20.42 5. Patrick Stevens ( Belgium ) 20.42 6. Michael Marsh ( U.S. ) 20.43 7. Ivan Garcia ( Cuba ) 20.45 8. Eric Wymeersch ( Belgium ) 20.84 9. Lamont Smith ( U.S. ) 21.08 Women 's 1,000 metres 1. Svetlana Masterkova ( Russia ) 2 minutes 28.98 seconds ( world record ) 2. Maria Mutola ( Mozambique ) 2:29.66 3. Malgorzata Rydz ( Poland ) 2:39.00 4. Anja Smolders ( Belgium ) 2:43.06 5. Veerle De Jaeghere ( Belgium ) 2:43.18 6. Eleonora Berlanda ( Italy ) 2:43.44 7. Anneke Matthijs ( Belgium ) 2:43.82 8. Jacqueline Martin ( Spain ) 2:44.22 Women 's 200 metres 1. Mary Onyali ( Nigeria ) 22.42 seconds 2. Inger Miller ( U.S. ) 22.66 3. Irina Privalova ( Russia ) 22.68 4. Natalia Voronova ( Russia ) 22.73 5. Marina Trandenkova ( Russia ) 22.84 6. Chandra Sturrup ( Bahamas ) 22.85 7. Zundra Feagin ( U.S. ) 23.18 8. Galina Malchugina ( Russia ) 23.25 Women 's 400 metres 1. Cathy Freeman ( Australia ) 49.48 seconds 2. Marie-Jose Perec ( France ) 49.72 3. Falilat Ogunkoya ( Nigeria ) 49.97 4. Pauline Davis ( Bahamas ) 50.14 5. Fatima Yussuf ( Nigeria ) 50.14 6. Maicel Malone ( U.S. ) 50.51 7. Hana Benesova ( Czech Republic ) 51.71 8. Ann Mercken ( Belgium ) 53.55 Men 's 3,000 metres 1. Daniel Komen ( Kenya ) 7 minutes 25.87 seconds 2. Khalid Boulami ( Morocco ) 7:31.65 3. Bob Kennedy ( U.S. ) 7:31.69 4. El Hassane Lahssini ( Morocco ) 7:32.44 5. Thomas Nyariki ( Kenya ) 7:35.56 6. Noureddine Morceli ( Algeria ) 7:36.81 7. Fita Bayesa ( Ethiopia ) 7:38.09 8. Martin Keino ( Kenya ) 7:38.88 Men 's discus 1. Lars Riedel ( Germany ) 66.74 metres 2. Anthony Washington ( U.S. ) 66.72 3. Vladimir Dubrovshchik ( Belarus ) 64.02 4. Virgilius Alekna ( Lithuania ) 63.62 5. Juergen Schult ( Germany ) 63.48 6. Vassiliy Kaptyukh ( Belarus ) 61.80 7. Vaclavas Kidikas ( Lithuania ) 60.92 8. Michael Mollenbeck ( Germany ) 59.24 Men 's triple jump 1. Jonathan Edwards ( Britain ) 17.50 metres 2. Yoelvis Quesada ( Cuba ) 17.29 3. Brian Wellman ( Bermuda ) 17.05 4. Kenny Harrison ( U.S. ) 16.97 5. Gennadi Markov ( Russia ) 16.66 6. Francis Agyepong ( Britain ) 16.63 7. Rogel Nachum ( Israel ) 16.36 8. Sigurd Njerve ( Norway ) 16.35 Men 's 1,500 metres 1. Hicham El Guerrouj ( Morocco ) three minutes 29.05 seconds 2. Isaac Viciosa ( Spain ) 3:33.00 3. William Tanui ( Kenya ) 3:33.36 4. Elijah Maru ( Kenya ) 3:33.64 5. Marcus O'Sullivan ( Ireland ) 3:33.77 6. John Mayock ( Britain ) 3:33.94 7. Laban Rotich ( Kenya ) 3:34.12 8. Christophe Impens ( Belgium ) 3:34.13 Women 's high jump 1. Stefka Kostadinova ( Bulgaria ) 2.03 metres 2. Inga Babakova ( Ukraine ) 2.03 3. Alina Astafei ( Germany ) 1.97 4. Tatyana Motkova ( Russia ) 1.94 5. Svetlana Zalevskaya ( Kazakhstan ) 1.91 6. Yelena Gulyayeva ( Russia ) 1.88 7. Hanna Haugland ( Norway ) 1.88 8 equal .\n",
      "max length of a sentence from train is 1232\n"
     ]
    }
   ],
   "source": [
    "maxlen=0\n",
    "for sent in sentences:\n",
    "    if len(sent)>maxlen:\n",
    "        maxlen=len(sent)\n",
    "        if maxlen>1200:\n",
    "            print(' '.join(sent))\n",
    "print('max length of a sentence from train is {0}'.format(maxlen))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(File,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "vocab=load_glove_model('word_embedding/glove.6B.50d/glove.6B.50d.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "sentences\n",
    "def sentence_embedding(sentences):\n",
    "    sents_embeddings=[]\n",
    "    for sent in sentences:\n",
    "        sent_embeddings=[]\n",
    "        for word in sent:\n",
    "            vector=vocab.get(str(word).lower())\n",
    "            if vector is not None:\n",
    "                sent_embeddings.append(vector)\n",
    "            else:\n",
    "                sent_embeddings.append(np.zeros(50))\n",
    "        sents_embeddings.append(sent_embeddings)\n",
    "    return sents_embeddings\n",
    "sents_embeddings=sentence_embedding(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 2.8050e-01,  9.6134e-02, -4.0411e-01, -4.3212e-01, -2.1813e-01,\n           3.9900e-01,  1.1994e-01, -5.8819e-01,  1.6138e-01, -9.0326e-01,\n           6.8040e-01,  7.9212e-02, -2.0700e-01,  8.6111e-01,  1.0581e+00,\n           5.9307e-01,  9.3522e-01, -1.1357e+00,  9.1209e-01, -7.3258e-01,\n           2.9839e-01, -6.5858e-01, -5.4395e-01,  1.2593e-01,  9.7543e-03,\n          -1.4733e+00,  9.9091e-01,  4.3032e-01, -9.9453e-01,  6.8398e-01,\n           3.0013e+00,  8.9257e-01, -1.1240e+00,  3.7326e-01, -1.1387e+00,\n          -1.0753e+00, -9.3879e-02,  1.1037e-01, -5.2445e-01, -3.6921e-01,\n          -1.1846e-01,  1.0811e-01,  1.6898e+00, -1.2099e+00, -3.5203e-01,\n           6.7106e-01, -3.0708e-01,  1.7115e+00,  2.9010e-01, -4.1397e-01],\n         [-1.5761e-01, -1.3796e-01, -4.2215e-01,  2.5714e-01,  2.7350e-01,\n           8.0252e-01,  7.5804e-01,  2.5174e-01, -1.1099e-02,  5.5110e-01,\n          -1.5435e-01, -8.2309e-02,  8.4994e-02, -2.3917e-01,  8.5194e-02,\n           4.7798e-01,  3.3177e-01, -3.8606e-01,  8.0990e-01,  3.0961e-01,\n           9.7102e-02,  3.6623e-01, -6.9415e-01,  2.5197e-01, -9.3049e-02,\n          -2.0751e+00,  4.5745e-01,  1.2630e-01, -5.2472e-01,  5.7085e-02,\n           9.6945e-01, -8.0068e-01, -1.5456e+00,  1.7291e-01, -3.8810e-01,\n          -9.9498e-01,  4.4053e-02, -1.1987e+00, -8.7563e-01, -9.4009e-01,\n           8.8661e-01, -7.4897e-02,  2.1447e-01, -3.7303e-01,  2.4727e-01,\n           4.3433e-01, -2.7533e-01,  1.5134e+00,  3.0387e-01, -1.9014e-01],\n         [ 2.9942e-01, -9.9670e-02, -1.2405e-03, -1.3147e+00,  2.9656e-01,\n           6.7304e-01, -6.5496e-01,  1.0273e-01, -1.4988e+00, -9.8879e-01,\n           1.4833e+00, -5.0715e-01, -7.3841e-01,  4.7889e-01,  3.9811e-01,\n          -6.0468e-01,  1.9186e-01, -1.9224e-02, -9.8668e-01,  1.4039e-01,\n           6.5313e-01, -7.7528e-01, -9.8685e-01,  6.2768e-01, -1.5063e-01,\n          -2.0823e+00, -9.4478e-01, -4.9395e-01, -8.0897e-01,  8.0822e-01,\n           2.6540e+00, -2.6725e-01,  1.7329e-01,  4.2389e-01, -8.8264e-01,\n          -3.9425e-01, -1.5823e-01,  5.8760e-01, -1.4896e-01, -8.2408e-02,\n           1.2181e+00, -3.4753e-01,  1.3133e+00, -1.2171e+00,  1.0170e+00,\n          -5.3184e-01, -7.3952e-02,  8.6380e-01,  1.4093e-02, -2.3443e-01],\n         [ 9.8201e-02,  3.9924e-01,  2.5697e-01, -8.5349e-02,  2.7175e-01,\n          -6.3637e-01, -6.2719e-01,  2.5895e-01, -5.3249e-01, -2.2927e-01,\n          -7.6258e-01,  2.1730e-01,  3.7017e-01,  8.2194e-02,  4.6016e-01,\n           1.4439e-01, -3.5333e-01, -6.2408e-01,  1.0250e-01, -5.8597e-01,\n           1.6874e-01,  4.1939e-01,  8.2275e-02,  4.8931e-01,  6.2348e-01,\n          -1.8434e+00, -1.1815e-01, -2.5465e-01,  3.8033e-01, -4.1893e-01,\n           3.0159e+00,  3.5014e-01, -1.2656e+00, -1.4951e-01, -3.2056e-01,\n          -7.2769e-01,  5.3980e-01, -1.2532e+00, -1.3795e-02, -4.8093e-03,\n           3.7453e-01,  4.1136e-01, -1.2614e-01,  4.8701e-01,  4.7820e-01,\n           3.5898e-01, -1.7090e-01,  7.0284e-01,  3.2207e-01,  7.7503e-01],\n         [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n           3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n           1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n           6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n           5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n          -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n           3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n          -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n          -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n           8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n         [-6.0099e-01, -5.4238e-01, -6.3388e-01,  1.8188e-01, -7.9334e-01,\n           1.6714e-01, -3.3029e-01,  5.7416e-01,  3.2505e-01,  4.8950e-01,\n          -2.7153e-01, -1.0610e+00, -3.5344e-01,  6.2698e-02,  7.4801e-01,\n           5.4313e-01,  3.6798e-01, -7.7952e-01,  3.1370e-01, -3.5480e-01,\n           7.6137e-01, -6.5797e-02, -2.0318e-01,  4.8621e-01, -6.9087e-01,\n          -1.1081e+00,  8.0227e-02, -4.1089e-01, -1.7475e-01,  6.5007e-01,\n           1.6209e+00,  9.5442e-01, -1.4747e+00,  5.3458e-02, -6.4396e-01,\n          -7.6953e-01,  3.6285e-01, -7.5213e-01, -1.0898e+00, -8.9911e-02,\n          -5.9198e-01, -6.3665e-02, -7.1984e-01, -2.4086e-01,  9.0157e-01,\n           5.6199e-01, -1.0045e+00,  7.7432e-01, -1.9462e-01, -3.7820e-01],\n         [-2.5236e-01, -2.6505e-02, -5.2338e-01, -4.7625e-01, -1.0816e-02,\n           4.9499e-01, -1.1493e+00,  2.2215e-01,  1.0172e-01, -1.1891e+00,\n           4.5989e-01,  3.4839e-01, -1.5586e-01,  2.6532e-01, -6.8722e-04,\n           1.1876e-01,  3.5669e-01, -3.0788e-02, -1.4578e+00, -1.2485e-01,\n           1.1846e+00,  8.2477e-01, -3.9040e-01,  2.5972e-01, -2.6186e-01,\n          -1.9816e+00, -8.0837e-01, -9.0312e-01, -5.3035e-01,  1.4572e+00,\n           2.7247e+00, -6.2139e-01, -4.6100e-02,  1.7324e-01,  3.6173e-01,\n          -1.8184e-01, -8.1845e-02, -5.9896e-01, -1.0192e+00, -7.5088e-01,\n           7.4903e-01,  1.0980e-01,  1.0343e+00, -6.7316e-01,  4.4687e-01,\n           9.9880e-02, -9.8741e-01,  5.8021e-02, -1.3698e-01, -3.9061e-01],\n         [-5.4579e-01, -2.9176e-01, -1.5682e+00, -6.8098e-01,  1.2828e+00,\n           8.3526e-01, -2.0424e-01, -3.0342e-01,  6.7128e-01, -8.1853e-01,\n          -3.2481e-01,  9.2006e-01,  9.0424e-01,  4.9587e-02,  4.7351e-01,\n           3.7749e-01,  4.8367e-01, -3.7928e-01, -3.1800e-01,  6.2878e-03,\n           3.3140e-01, -4.4932e-01,  8.6401e-01, -1.2623e-02,  9.3789e-03,\n          -4.9651e-01,  2.5949e-01,  4.6834e-01,  4.1870e-01, -6.2030e-01,\n           1.2316e+00, -3.5936e-01,  4.5267e-01,  1.6958e+00, -1.7503e-01,\n           3.7319e-01,  2.9452e-01,  5.2497e-01,  4.6051e-01,  4.1741e-02,\n           5.1848e-01,  7.4013e-01, -4.6537e-01, -3.7915e-01,  8.8908e-01,\n           1.2535e+00,  1.5293e-02, -2.4534e-01, -9.8459e-01,  7.8613e-01],\n         [ 1.5164e-01,  3.0177e-01, -1.6763e-01,  1.7684e-01,  3.1719e-01,\n           3.3973e-01, -4.3478e-01, -3.1086e-01, -4.4999e-01, -2.9486e-01,\n           1.6608e-01,  1.1963e-01, -4.1328e-01, -4.2353e-01,  5.9868e-01,\n           2.8825e-01, -1.1547e-01, -4.1848e-02, -6.7989e-01, -2.5063e-01,\n           1.8472e-01,  8.6876e-02,  4.6582e-01,  1.5035e-02,  4.3474e-02,\n          -1.4671e+00, -3.0384e-01, -2.3441e-02,  3.0589e-01, -2.1785e-01,\n           3.7460e+00,  4.2284e-03, -1.8436e-01, -4.6209e-01,  9.8329e-02,\n          -1.1907e-01,  2.3919e-01,  1.1610e-01,  4.1705e-01,  5.6763e-02,\n          -6.3681e-05,  6.8987e-02,  8.7939e-02, -1.0285e-01, -1.3931e-01,\n           2.2314e-01, -8.0803e-02, -3.5652e-01,  1.6413e-02,  1.0216e-01]]])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(sents_embeddings[:1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}